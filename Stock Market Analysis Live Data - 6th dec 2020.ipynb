{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Market Prediction And Forecasting Using Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras and Tensorflow >2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Collection\n",
    "import pandas_datareader as pdr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"57d73181a4657ccec0f4a2907847ffb3207c7fec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(r\"C:/Users/sagar/joyjeet sir's notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.get_data_tiingo('AAPL', api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('AAPL.csv')  # we can fetch only 50 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-12-08 00:00:00+00:00</td>\n",
       "      <td>118.23</td>\n",
       "      <td>118.60</td>\n",
       "      <td>116.860</td>\n",
       "      <td>117.52</td>\n",
       "      <td>34309450</td>\n",
       "      <td>27.380718</td>\n",
       "      <td>27.466406</td>\n",
       "      <td>27.063442</td>\n",
       "      <td>27.216290</td>\n",
       "      <td>137237800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-12-09 00:00:00+00:00</td>\n",
       "      <td>115.62</td>\n",
       "      <td>117.69</td>\n",
       "      <td>115.080</td>\n",
       "      <td>117.64</td>\n",
       "      <td>46361357</td>\n",
       "      <td>26.776272</td>\n",
       "      <td>27.255660</td>\n",
       "      <td>26.651214</td>\n",
       "      <td>27.244081</td>\n",
       "      <td>185445428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-12-10 00:00:00+00:00</td>\n",
       "      <td>116.17</td>\n",
       "      <td>116.94</td>\n",
       "      <td>115.510</td>\n",
       "      <td>116.04</td>\n",
       "      <td>29212727</td>\n",
       "      <td>26.903646</td>\n",
       "      <td>27.081969</td>\n",
       "      <td>26.750797</td>\n",
       "      <td>26.873539</td>\n",
       "      <td>116850908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-12-11 00:00:00+00:00</td>\n",
       "      <td>113.18</td>\n",
       "      <td>115.39</td>\n",
       "      <td>112.851</td>\n",
       "      <td>115.19</td>\n",
       "      <td>46886161</td>\n",
       "      <td>26.211196</td>\n",
       "      <td>26.723006</td>\n",
       "      <td>26.135003</td>\n",
       "      <td>26.676689</td>\n",
       "      <td>187544644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-12-14 00:00:00+00:00</td>\n",
       "      <td>112.48</td>\n",
       "      <td>112.68</td>\n",
       "      <td>109.790</td>\n",
       "      <td>112.18</td>\n",
       "      <td>65003609</td>\n",
       "      <td>26.049084</td>\n",
       "      <td>26.095401</td>\n",
       "      <td>25.426110</td>\n",
       "      <td>25.979607</td>\n",
       "      <td>260014436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                       date   close    high      low    open  \\\n",
       "0   AAPL  2015-12-08 00:00:00+00:00  118.23  118.60  116.860  117.52   \n",
       "1   AAPL  2015-12-09 00:00:00+00:00  115.62  117.69  115.080  117.64   \n",
       "2   AAPL  2015-12-10 00:00:00+00:00  116.17  116.94  115.510  116.04   \n",
       "3   AAPL  2015-12-11 00:00:00+00:00  113.18  115.39  112.851  115.19   \n",
       "4   AAPL  2015-12-14 00:00:00+00:00  112.48  112.68  109.790  112.18   \n",
       "\n",
       "     volume   adjClose    adjHigh     adjLow    adjOpen  adjVolume  divCash  \\\n",
       "0  34309450  27.380718  27.466406  27.063442  27.216290  137237800      0.0   \n",
       "1  46361357  26.776272  27.255660  26.651214  27.244081  185445428      0.0   \n",
       "2  29212727  26.903646  27.081969  26.750797  26.873539  116850908      0.0   \n",
       "3  46886161  26.211196  26.723006  26.135003  26.676689  187544644      0.0   \n",
       "4  65003609  26.049084  26.095401  25.426110  25.979607  260014436      0.0   \n",
       "\n",
       "   splitFactor  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-11-30 00:00:00+00:00</td>\n",
       "      <td>119.05</td>\n",
       "      <td>120.9700</td>\n",
       "      <td>116.81</td>\n",
       "      <td>116.97</td>\n",
       "      <td>169410176</td>\n",
       "      <td>119.05</td>\n",
       "      <td>120.9700</td>\n",
       "      <td>116.81</td>\n",
       "      <td>116.97</td>\n",
       "      <td>169410176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-12-01 00:00:00+00:00</td>\n",
       "      <td>122.72</td>\n",
       "      <td>123.4693</td>\n",
       "      <td>120.01</td>\n",
       "      <td>121.01</td>\n",
       "      <td>125920963</td>\n",
       "      <td>122.72</td>\n",
       "      <td>123.4693</td>\n",
       "      <td>120.01</td>\n",
       "      <td>121.01</td>\n",
       "      <td>125920963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-12-02 00:00:00+00:00</td>\n",
       "      <td>123.08</td>\n",
       "      <td>123.3700</td>\n",
       "      <td>120.89</td>\n",
       "      <td>122.02</td>\n",
       "      <td>89004195</td>\n",
       "      <td>123.08</td>\n",
       "      <td>123.3700</td>\n",
       "      <td>120.89</td>\n",
       "      <td>122.02</td>\n",
       "      <td>89004195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-12-03 00:00:00+00:00</td>\n",
       "      <td>122.94</td>\n",
       "      <td>123.7800</td>\n",
       "      <td>122.21</td>\n",
       "      <td>123.52</td>\n",
       "      <td>78967630</td>\n",
       "      <td>122.94</td>\n",
       "      <td>123.7800</td>\n",
       "      <td>122.21</td>\n",
       "      <td>123.52</td>\n",
       "      <td>78967630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-12-04 00:00:00+00:00</td>\n",
       "      <td>122.25</td>\n",
       "      <td>122.8608</td>\n",
       "      <td>121.52</td>\n",
       "      <td>122.60</td>\n",
       "      <td>78260421</td>\n",
       "      <td>122.25</td>\n",
       "      <td>122.8608</td>\n",
       "      <td>121.52</td>\n",
       "      <td>122.60</td>\n",
       "      <td>78260421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol                       date   close      high     low    open  \\\n",
       "1253   AAPL  2020-11-30 00:00:00+00:00  119.05  120.9700  116.81  116.97   \n",
       "1254   AAPL  2020-12-01 00:00:00+00:00  122.72  123.4693  120.01  121.01   \n",
       "1255   AAPL  2020-12-02 00:00:00+00:00  123.08  123.3700  120.89  122.02   \n",
       "1256   AAPL  2020-12-03 00:00:00+00:00  122.94  123.7800  122.21  123.52   \n",
       "1257   AAPL  2020-12-04 00:00:00+00:00  122.25  122.8608  121.52  122.60   \n",
       "\n",
       "         volume  adjClose   adjHigh  adjLow  adjOpen  adjVolume  divCash  \\\n",
       "1253  169410176    119.05  120.9700  116.81   116.97  169410176      0.0   \n",
       "1254  125920963    122.72  123.4693  120.01   121.01  125920963      0.0   \n",
       "1255   89004195    123.08  123.3700  120.89   122.02   89004195      0.0   \n",
       "1256   78967630    122.94  123.7800  122.21   123.52   78967630      0.0   \n",
       "1257   78260421    122.25  122.8608  121.52   122.60   78260421      0.0   \n",
       "\n",
       "      splitFactor  \n",
       "1253          1.0  \n",
       "1254          1.0  \n",
       "1255          1.0  \n",
       "1256          1.0  \n",
       "1257          1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Price  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.reset_index()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       118.23\n",
       "1       115.62\n",
       "2       116.17\n",
       "3       113.18\n",
       "4       112.48\n",
       "         ...  \n",
       "1253    119.05\n",
       "1254    122.72\n",
       "1255    123.08\n",
       "1256    122.94\n",
       "1257    122.25\n",
       "Name: close, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e4cb9ae048>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM are sensitive to the scale of the data. so we apply MinMax scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       118.23\n",
       "1       115.62\n",
       "2       116.17\n",
       "3       113.18\n",
       "4       112.48\n",
       "         ...  \n",
       "1253    119.05\n",
       "1254    122.72\n",
       "1255    123.08\n",
       "1256    122.94\n",
       "1257    122.25\n",
       "Name: close, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))  # transfer between 0 to 1 \n",
    "df1=scaler.fit_transform(np.array(df1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06708358]\n",
      " [0.06080577]\n",
      " [0.06212868]\n",
      " ...\n",
      " [0.07874925]\n",
      " [0.07841251]\n",
      " [0.07675286]]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##splitting dataset into train and test split\n",
    "training_size=int(len(df1)*0.65)   ### i am just taking the train size == 1258 * 0.65  == 817\n",
    "test_size=len(df1)-training_size   ### length of total length - train size    == 441\n",
    "train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]  ### take data from 0 to 817 as train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 441)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06708358],\n",
       "       [0.06080577],\n",
       "       [0.06212868],\n",
       "       [0.05493686],\n",
       "       [0.05325316],\n",
       "       [0.04846663],\n",
       "       [0.05051112],\n",
       "       [0.04483464],\n",
       "       [0.03773903],\n",
       "       [0.0408659 ],\n",
       "       [0.04062538],\n",
       "       [0.04394468],\n",
       "       [0.04254961],\n",
       "       [0.03963921],\n",
       "       [0.04425737],\n",
       "       [0.04084185],\n",
       "       [0.03588695],\n",
       "       [0.03610343],\n",
       "       [0.02975346],\n",
       "       [0.02491882],\n",
       "       [0.01469633],\n",
       "       [0.01592303],\n",
       "       [0.01969934],\n",
       "       [0.02313891],\n",
       "       [0.01695731],\n",
       "       [0.02208058],\n",
       "       [0.01633193],\n",
       "       [0.01520144],\n",
       "       [0.01551413],\n",
       "       [0.01433554],\n",
       "       [0.02665063],\n",
       "       [0.02188815],\n",
       "       [0.02321106],\n",
       "       [0.0074083 ],\n",
       "       [0.00901984],\n",
       "       [0.01683704],\n",
       "       [0.01464823],\n",
       "       [0.00995791],\n",
       "       [0.0144558 ],\n",
       "       [0.01505713],\n",
       "       [0.00885147],\n",
       "       [0.01123271],\n",
       "       [0.01118461],\n",
       "       [0.0094528 ],\n",
       "       [0.00808178],\n",
       "       [0.00877931],\n",
       "       [0.01515334],\n",
       "       [0.01871317],\n",
       "       [0.01423933],\n",
       "       [0.01371016],\n",
       "       [0.01573061],\n",
       "       [0.01046302],\n",
       "       [0.01385448],\n",
       "       [0.01544197],\n",
       "       [0.01580277],\n",
       "       [0.0152736 ],\n",
       "       [0.02450992],\n",
       "       [0.02503909],\n",
       "       [0.02684305],\n",
       "       [0.03047505],\n",
       "       [0.02773301],\n",
       "       [0.02571257],\n",
       "       [0.02592904],\n",
       "       [0.02604931],\n",
       "       [0.02867108],\n",
       "       [0.02929645],\n",
       "       [0.03425135],\n",
       "       [0.03759471],\n",
       "       [0.03718581],\n",
       "       [0.03747444],\n",
       "       [0.03745039],\n",
       "       [0.03939868],\n",
       "       [0.03797956],\n",
       "       [0.03687312],\n",
       "       [0.03571858],\n",
       "       [0.04170776],\n",
       "       [0.04622971],\n",
       "       [0.04485869],\n",
       "       [0.04726398],\n",
       "       [0.04998196],\n",
       "       [0.04683103],\n",
       "       [0.04959711],\n",
       "       [0.04377631],\n",
       "       [0.04406494],\n",
       "       [0.04493085],\n",
       "       [0.04834636],\n",
       "       [0.05219483],\n",
       "       [0.05233915],\n",
       "       [0.04692724],\n",
       "       [0.0412267 ],\n",
       "       [0.03985568],\n",
       "       [0.04038485],\n",
       "       [0.03759471],\n",
       "       [0.03689717],\n",
       "       [0.035454  ],\n",
       "       [0.03369814],\n",
       "       [0.01799158],\n",
       "       [0.01079976],\n",
       "       [0.00817799],\n",
       "       [0.00793746],\n",
       "       [0.01164161],\n",
       "       [0.00926037],\n",
       "       [0.00697535],\n",
       "       [0.00572459],\n",
       "       [0.00589296],\n",
       "       [0.0074083 ],\n",
       "       [0.00521948],\n",
       "       [0.        ],\n",
       "       [0.00043295],\n",
       "       [0.00851473],\n",
       "       [0.00757667],\n",
       "       [0.01015033],\n",
       "       [0.00928443],\n",
       "       [0.01173782],\n",
       "       [0.01464823],\n",
       "       [0.018184  ],\n",
       "       [0.02232111],\n",
       "       [0.02422129],\n",
       "       [0.02407697],\n",
       "       [0.02289838],\n",
       "       [0.01953097],\n",
       "       [0.01775105],\n",
       "       [0.01823211],\n",
       "       [0.01993987],\n",
       "       [0.02090198],\n",
       "       [0.02068551],\n",
       "       [0.02239327],\n",
       "       [0.02042093],\n",
       "       [0.01683704],\n",
       "       [0.01712568],\n",
       "       [0.01635598],\n",
       "       [0.01734215],\n",
       "       [0.01200241],\n",
       "       [0.01144919],\n",
       "       [0.01339747],\n",
       "       [0.01253157],\n",
       "       [0.01385448],\n",
       "       [0.00736019],\n",
       "       [0.004089  ],\n",
       "       [0.0078172 ],\n",
       "       [0.00976548],\n",
       "       [0.01265183],\n",
       "       [0.01334937],\n",
       "       [0.01118461],\n",
       "       [0.01248346],\n",
       "       [0.01346963],\n",
       "       [0.01524955],\n",
       "       [0.01597114],\n",
       "       [0.01702946],\n",
       "       [0.01570655],\n",
       "       [0.02032471],\n",
       "       [0.02030066],\n",
       "       [0.02282622],\n",
       "       [0.02292243],\n",
       "       [0.02313891],\n",
       "       [0.0218641 ],\n",
       "       [0.02001203],\n",
       "       [0.01683704],\n",
       "       [0.0152255 ],\n",
       "       [0.03033073],\n",
       "       [0.03367408],\n",
       "       [0.0333614 ],\n",
       "       [0.03778713],\n",
       "       [0.03401082],\n",
       "       [0.03716176],\n",
       "       [0.03735418],\n",
       "       [0.0412267 ],\n",
       "       [0.04336741],\n",
       "       [0.04442574],\n",
       "       [0.04247745],\n",
       "       [0.04230908],\n",
       "       [0.0429104 ],\n",
       "       [0.04603728],\n",
       "       [0.04579675],\n",
       "       [0.04541191],\n",
       "       [0.04507517],\n",
       "       [0.04574865],\n",
       "       [0.04370415],\n",
       "       [0.04452195],\n",
       "       [0.04254961],\n",
       "       [0.04144317],\n",
       "       [0.03992784],\n",
       "       [0.03963921],\n",
       "       [0.03766687],\n",
       "       [0.0379074 ],\n",
       "       [0.03942273],\n",
       "       [0.04182802],\n",
       "       [0.04175586],\n",
       "       [0.04334336],\n",
       "       [0.03651233],\n",
       "       [0.03076368],\n",
       "       [0.0363199 ],\n",
       "       [0.04235719],\n",
       "       [0.0515454 ],\n",
       "       [0.06068551],\n",
       "       [0.05912207],\n",
       "       [0.05589898],\n",
       "       [0.05587492],\n",
       "       [0.05582682],\n",
       "       [0.05840048],\n",
       "       [0.05380637],\n",
       "       [0.05421527],\n",
       "       [0.05472038],\n",
       "       [0.05678894],\n",
       "       [0.05253157],\n",
       "       [0.05462417],\n",
       "       [0.05334937],\n",
       "       [0.05450391],\n",
       "       [0.05462417],\n",
       "       [0.05664462],\n",
       "       [0.05705352],\n",
       "       [0.06184005],\n",
       "       [0.06244137],\n",
       "       [0.06494287],\n",
       "       [0.06407697],\n",
       "       [0.06564041],\n",
       "       [0.06544799],\n",
       "       [0.06525556],\n",
       "       [0.06441371],\n",
       "       [0.06426939],\n",
       "       [0.06316296],\n",
       "       [0.06568851],\n",
       "       [0.06713169],\n",
       "       [0.06073361],\n",
       "       [0.05806374],\n",
       "       [0.05623572],\n",
       "       [0.05580277],\n",
       "       [0.05087192],\n",
       "       [0.05111245],\n",
       "       [0.04687913],\n",
       "       [0.0444979 ],\n",
       "       [0.0482742 ],\n",
       "       [0.04983764],\n",
       "       [0.04940469],\n",
       "       [0.04197234],\n",
       "       [0.04351173],\n",
       "       [0.03696933],\n",
       "       [0.04033674],\n",
       "       [0.04726398],\n",
       "       [0.04716777],\n",
       "       [0.04743235],\n",
       "       [0.05144919],\n",
       "       [0.05161756],\n",
       "       [0.05024654],\n",
       "       [0.05159351],\n",
       "       [0.05106434],\n",
       "       [0.05079976],\n",
       "       [0.04853879],\n",
       "       [0.04606133],\n",
       "       [0.0470475 ],\n",
       "       [0.04514732],\n",
       "       [0.04716777],\n",
       "       [0.04976548],\n",
       "       [0.05238725],\n",
       "       [0.05678894],\n",
       "       [0.0552255 ],\n",
       "       [0.0597715 ],\n",
       "       [0.0597715 ],\n",
       "       [0.06128683],\n",
       "       [0.06164762],\n",
       "       [0.06325917],\n",
       "       [0.06400481],\n",
       "       [0.06426939],\n",
       "       [0.06241732],\n",
       "       [0.06297054],\n",
       "       [0.06475045],\n",
       "       [0.06354781],\n",
       "       [0.06347565],\n",
       "       [0.06128683],\n",
       "       [0.06208058],\n",
       "       [0.06176789],\n",
       "       [0.06318701],\n",
       "       [0.06631389],\n",
       "       [0.06891161],\n",
       "       [0.06920024],\n",
       "       [0.07073963],\n",
       "       [0.06953698],\n",
       "       [0.06903187],\n",
       "       [0.07134095],\n",
       "       [0.0713169 ],\n",
       "       [0.07081179],\n",
       "       [0.07134095],\n",
       "       [0.07153337],\n",
       "       [0.07126879],\n",
       "       [0.0758629 ],\n",
       "       [0.07600722],\n",
       "       [0.07603127],\n",
       "       [0.07526158],\n",
       "       [0.07458809],\n",
       "       [0.09238725],\n",
       "       [0.09185809],\n",
       "       [0.093181  ],\n",
       "       [0.0960914 ],\n",
       "       [0.09907396],\n",
       "       [0.10030066],\n",
       "       [0.10121467],\n",
       "       [0.10049308],\n",
       "       [0.10330728],\n",
       "       [0.10746843],\n",
       "       [0.10864702],\n",
       "       [0.10825015],\n",
       "       [0.10915213],\n",
       "       [0.11150932],\n",
       "       [0.11249549],\n",
       "       [0.11110042],\n",
       "       [0.11141311],\n",
       "       [0.11206254],\n",
       "       [0.11220686],\n",
       "       [0.11894167],\n",
       "       [0.11694528],\n",
       "       [0.11891762],\n",
       "       [0.11785929],\n",
       "       [0.11829224],\n",
       "       [0.11704149],\n",
       "       [0.1162718 ],\n",
       "       [0.11737823],\n",
       "       [0.11752255],\n",
       "       [0.11701744],\n",
       "       [0.12055322],\n",
       "       [0.12110643],\n",
       "       [0.11942273],\n",
       "       [0.12295851],\n",
       "       [0.11906194],\n",
       "       [0.1228623 ],\n",
       "       [0.12165965],\n",
       "       [0.12098617],\n",
       "       [0.12156344],\n",
       "       [0.12858689],\n",
       "       [0.12935658],\n",
       "       [0.12889958],\n",
       "       [0.12825015],\n",
       "       [0.12834636],\n",
       "       [0.13092002],\n",
       "       [0.12911606],\n",
       "       [0.12825015],\n",
       "       [0.12748046],\n",
       "       [0.12707156],\n",
       "       [0.12336741],\n",
       "       [0.12377631],\n",
       "       [0.12197234],\n",
       "       [0.12384847],\n",
       "       [0.12233313],\n",
       "       [0.12108238],\n",
       "       [0.12531569],\n",
       "       [0.12490679],\n",
       "       [0.12820204],\n",
       "       [0.13034275],\n",
       "       [0.12829826],\n",
       "       [0.12856284],\n",
       "       [0.1282261 ],\n",
       "       [0.1352736 ],\n",
       "       [0.13751052],\n",
       "       [0.13642814],\n",
       "       [0.13515334],\n",
       "       [0.1409982 ],\n",
       "       [0.15073963],\n",
       "       [0.15309681],\n",
       "       [0.15134095],\n",
       "       [0.1530006 ],\n",
       "       [0.15817198],\n",
       "       [0.15720986],\n",
       "       [0.15665664],\n",
       "       [0.14410102],\n",
       "       [0.14960914],\n",
       "       [0.15085989],\n",
       "       [0.15309681],\n",
       "       [0.15263981],\n",
       "       [0.15153337],\n",
       "       [0.15280818],\n",
       "       [0.1521828 ],\n",
       "       [0.15232712],\n",
       "       [0.1501383 ],\n",
       "       [0.15114853],\n",
       "       [0.15660854],\n",
       "       [0.1529525 ],\n",
       "       [0.15420325],\n",
       "       [0.15641612],\n",
       "       [0.1555021 ],\n",
       "       [0.1410463 ],\n",
       "       [0.13248346],\n",
       "       [0.13529765],\n",
       "       [0.13185809],\n",
       "       [0.12976548],\n",
       "       [0.12490679],\n",
       "       [0.13469633],\n",
       "       [0.13149729],\n",
       "       [0.13356584],\n",
       "       [0.13298857],\n",
       "       [0.13455201],\n",
       "       [0.13344558],\n",
       "       [0.12841852],\n",
       "       [0.13346963],\n",
       "       [0.12829826],\n",
       "       [0.12911606],\n",
       "       [0.1278653 ],\n",
       "       [0.12928443],\n",
       "       [0.12601323],\n",
       "       [0.1295009 ],\n",
       "       [0.13161756],\n",
       "       [0.13274805],\n",
       "       [0.13325316],\n",
       "       [0.1381359 ],\n",
       "       [0.14119062],\n",
       "       [0.14244137],\n",
       "       [0.14369212],\n",
       "       [0.1459531 ],\n",
       "       [0.1443175 ],\n",
       "       [0.14414913],\n",
       "       [0.14852676],\n",
       "       [0.1500902 ],\n",
       "       [0.15182201],\n",
       "       [0.14484666],\n",
       "       [0.14229705],\n",
       "       [0.14044498],\n",
       "       [0.14361996],\n",
       "       [0.16067348],\n",
       "       [0.15689717],\n",
       "       [0.15886951],\n",
       "       [0.16469032],\n",
       "       [0.16774504],\n",
       "       [0.17010222],\n",
       "       [0.15629585],\n",
       "       [0.16149128],\n",
       "       [0.16719182],\n",
       "       [0.17140108],\n",
       "       [0.16983764],\n",
       "       [0.16240529],\n",
       "       [0.16153939],\n",
       "       [0.16084185],\n",
       "       [0.16702345],\n",
       "       [0.16750451],\n",
       "       [0.16579675],\n",
       "       [0.16721587],\n",
       "       [0.17108839],\n",
       "       [0.17455201],\n",
       "       [0.17561034],\n",
       "       [0.17717378],\n",
       "       [0.17729405],\n",
       "       [0.17255562],\n",
       "       [0.17214672],\n",
       "       [0.17058328],\n",
       "       [0.16425737],\n",
       "       [0.17116055],\n",
       "       [0.16962117],\n",
       "       [0.16671076],\n",
       "       [0.16341551],\n",
       "       [0.16726398],\n",
       "       [0.16435358],\n",
       "       [0.1644979 ],\n",
       "       [0.15809982],\n",
       "       [0.15165364],\n",
       "       [0.1480457 ],\n",
       "       [0.14482261],\n",
       "       [0.15105232],\n",
       "       [0.15367408],\n",
       "       [0.15138906],\n",
       "       [0.1534095 ],\n",
       "       [0.15266386],\n",
       "       [0.15427541],\n",
       "       [0.15187011],\n",
       "       [0.15646422],\n",
       "       [0.15624775],\n",
       "       [0.1575466 ],\n",
       "       [0.15769092],\n",
       "       [0.15925436],\n",
       "       [0.15793145],\n",
       "       [0.16031269],\n",
       "       [0.16726398],\n",
       "       [0.1686831 ],\n",
       "       [0.16697535],\n",
       "       [0.15788334],\n",
       "       [0.15853277],\n",
       "       [0.15834035],\n",
       "       [0.16057727],\n",
       "       [0.15891762],\n",
       "       [0.16132291],\n",
       "       [0.17488876],\n",
       "       [0.18371618],\n",
       "       [0.18929645],\n",
       "       [0.18412508],\n",
       "       [0.18705953],\n",
       "       [0.19761876],\n",
       "       [0.20182802],\n",
       "       [0.20317498],\n",
       "       [0.20661455],\n",
       "       [0.20574865],\n",
       "       [0.20283824],\n",
       "       [0.20115454],\n",
       "       [0.19482862],\n",
       "       [0.18939266],\n",
       "       [0.19425135],\n",
       "       [0.19196633],\n",
       "       [0.19155743],\n",
       "       [0.19915815],\n",
       "       [0.20353578],\n",
       "       [0.20355983],\n",
       "       [0.20144317],\n",
       "       [0.19898978],\n",
       "       [0.19035478],\n",
       "       [0.19605532],\n",
       "       [0.19413109],\n",
       "       [0.19112447],\n",
       "       [0.19073963],\n",
       "       [0.18922429],\n",
       "       [0.18996993],\n",
       "       [0.1900902 ],\n",
       "       [0.19802766],\n",
       "       [0.19569453],\n",
       "       [0.19706554],\n",
       "       [0.19694528],\n",
       "       [0.20115454],\n",
       "       [0.2070475 ],\n",
       "       [0.20252556],\n",
       "       [0.20206855],\n",
       "       [0.20365604],\n",
       "       [0.20365604],\n",
       "       [0.19297655],\n",
       "       [0.19304871],\n",
       "       [0.19420325],\n",
       "       [0.18975346],\n",
       "       [0.19704149],\n",
       "       [0.19696933],\n",
       "       [0.19889357],\n",
       "       [0.20363199],\n",
       "       [0.20206855],\n",
       "       [0.20202044],\n",
       "       [0.20192423],\n",
       "       [0.20430547],\n",
       "       [0.20865905],\n",
       "       [0.20649429],\n",
       "       [0.21349369],\n",
       "       [0.21387853],\n",
       "       [0.2119543 ],\n",
       "       [0.20844257],\n",
       "       [0.20853879],\n",
       "       [0.20175586],\n",
       "       [0.19427541],\n",
       "       [0.19523752],\n",
       "       [0.18669874],\n",
       "       [0.1843175 ],\n",
       "       [0.18542393],\n",
       "       [0.18626578],\n",
       "       [0.16875526],\n",
       "       [0.15911004],\n",
       "       [0.17484065],\n",
       "       [0.16644618],\n",
       "       [0.15588695],\n",
       "       [0.15891762],\n",
       "       [0.17407096],\n",
       "       [0.17799158],\n",
       "       [0.18527962],\n",
       "       [0.19879735],\n",
       "       [0.19745039],\n",
       "       [0.19605532],\n",
       "       [0.19417919],\n",
       "       [0.19761876],\n",
       "       [0.20483464],\n",
       "       [0.213181  ],\n",
       "       [0.21178593],\n",
       "       [0.2111365 ],\n",
       "       [0.20363199],\n",
       "       [0.20654239],\n",
       "       [0.20800962],\n",
       "       [0.20764883],\n",
       "       [0.20370415],\n",
       "       [0.20829826],\n",
       "       [0.21561034],\n",
       "       [0.21979555],\n",
       "       [0.21558629],\n",
       "       [0.21190619],\n",
       "       [0.2124113 ],\n",
       "       [0.21089597],\n",
       "       [0.20435358],\n",
       "       [0.20420926],\n",
       "       [0.19466025],\n",
       "       [0.18883945],\n",
       "       [0.17943476],\n",
       "       [0.19826819],\n",
       "       [0.18761275],\n",
       "       [0.18313891],\n",
       "       [0.18626578],\n",
       "       [0.18361996],\n",
       "       [0.18773301],\n",
       "       [0.19547805],\n",
       "       [0.19834035],\n",
       "       [0.18770896],\n",
       "       [0.1917258 ],\n",
       "       [0.19942273],\n",
       "       [0.19747444],\n",
       "       [0.20156344],\n",
       "       [0.20298256],\n",
       "       [0.20560433],\n",
       "       [0.21142514],\n",
       "       [0.21046302],\n",
       "       [0.19834035],\n",
       "       [0.18131088],\n",
       "       [0.18015634],\n",
       "       [0.17462417],\n",
       "       [0.17633193],\n",
       "       [0.17770295],\n",
       "       [0.17313289],\n",
       "       [0.18020445],\n",
       "       [0.18944077],\n",
       "       [0.2074083 ],\n",
       "       [0.20817799],\n",
       "       [0.22487072],\n",
       "       [0.22806975],\n",
       "       [0.23021046],\n",
       "       [0.2333614 ],\n",
       "       [0.23980758],\n",
       "       [0.2363199 ],\n",
       "       [0.23526158],\n",
       "       [0.23114853],\n",
       "       [0.23533373],\n",
       "       [0.23247144],\n",
       "       [0.23083584],\n",
       "       [0.23401082],\n",
       "       [0.23288034],\n",
       "       [0.23576669],\n",
       "       [0.23526158],\n",
       "       [0.23629585],\n",
       "       [0.23466025],\n",
       "       [0.23369814],\n",
       "       [0.2321828 ],\n",
       "       [0.24028863],\n",
       "       [0.24411305],\n",
       "       [0.24767288],\n",
       "       [0.24928443],\n",
       "       [0.24803367],\n",
       "       [0.24380036],\n",
       "       [0.24266987],\n",
       "       [0.24519543],\n",
       "       [0.24139507],\n",
       "       [0.2416356 ],\n",
       "       [0.23692123],\n",
       "       [0.2366807 ],\n",
       "       [0.22934456],\n",
       "       [0.23129284],\n",
       "       [0.22879134],\n",
       "       [0.22749248],\n",
       "       [0.22087793],\n",
       "       [0.22631389],\n",
       "       [0.22566446],\n",
       "       [0.22888755],\n",
       "       [0.22794949],\n",
       "       [0.23292844],\n",
       "       [0.22508719],\n",
       "       [0.22864702],\n",
       "       [0.23482862],\n",
       "       [0.24110643],\n",
       "       [0.24055322],\n",
       "       [0.23461215],\n",
       "       [0.24218882],\n",
       "       [0.2429104 ],\n",
       "       [0.24190018],\n",
       "       [0.24319904],\n",
       "       [0.24067348],\n",
       "       [0.24423331],\n",
       "       [0.24317498],\n",
       "       [0.24358388],\n",
       "       [0.24692724],\n",
       "       [0.25130487],\n",
       "       [0.24983764],\n",
       "       [0.24206855],\n",
       "       [0.23949489],\n",
       "       [0.2404089 ],\n",
       "       [0.26737222],\n",
       "       [0.28153939],\n",
       "       [0.28298256],\n",
       "       [0.28558028],\n",
       "       [0.2808659 ],\n",
       "       [0.28120265],\n",
       "       [0.28512327],\n",
       "       [0.28187613],\n",
       "       [0.28509922],\n",
       "       [0.28721587],\n",
       "       [0.28839447],\n",
       "       [0.29580277],\n",
       "       [0.30604931],\n",
       "       [0.30095009],\n",
       "       [0.29993987],\n",
       "       [0.29996392],\n",
       "       [0.30102225],\n",
       "       [0.30263379],\n",
       "       [0.30691521],\n",
       "       [0.31114853],\n",
       "       [0.31903788],\n",
       "       [0.32396873],\n",
       "       [0.33022249],\n",
       "       [0.33197835],\n",
       "       [0.32839447],\n",
       "       [0.31932652],\n",
       "       [0.31499699],\n",
       "       [0.30785328],\n",
       "       [0.32113049],\n",
       "       [0.31444378],\n",
       "       [0.32728803],\n",
       "       [0.32110643],\n",
       "       [0.3067709 ],\n",
       "       [0.3076368 ],\n",
       "       [0.30794949],\n",
       "       [0.31194227],\n",
       "       [0.30624173],\n",
       "       [0.31377029],\n",
       "       [0.3171377 ],\n",
       "       [0.31288034],\n",
       "       [0.32377631],\n",
       "       [0.32567649],\n",
       "       [0.32933253],\n",
       "       [0.33419122],\n",
       "       [0.34090198],\n",
       "       [0.33108839],\n",
       "       [0.32218882],\n",
       "       [0.32093806],\n",
       "       [0.32839447],\n",
       "       [0.30311485],\n",
       "       [0.29852075],\n",
       "       [0.31694528],\n",
       "       [0.30552014],\n",
       "       [0.31704149],\n",
       "       [0.31473241],\n",
       "       [0.30229705],\n",
       "       [0.31021046],\n",
       "       [0.31343355],\n",
       "       [0.31843656],\n",
       "       [0.30006013],\n",
       "       [0.31138906],\n",
       "       [0.30297054],\n",
       "       [0.29320505],\n",
       "       [0.29575466],\n",
       "       [0.30912808],\n",
       "       [0.31720986],\n",
       "       [0.28175586],\n",
       "       [0.2675887 ],\n",
       "       [0.27283223],\n",
       "       [0.28769693],\n",
       "       [0.28418521],\n",
       "       [0.27451594],\n",
       "       [0.24974143],\n",
       "       [0.24507517],\n",
       "       [0.23201443],\n",
       "       [0.24310283],\n",
       "       [0.24820204],\n",
       "       [0.22975346],\n",
       "       [0.20839447],\n",
       "       [0.20791341],\n",
       "       [0.19711365],\n",
       "       [0.20271798],\n",
       "       [0.20180397],\n",
       "       [0.21791942],\n",
       "       [0.21457607],\n",
       "       [0.21224293],\n",
       "       [0.22725195],\n",
       "       [0.20769693],\n",
       "       [0.20295851],\n",
       "       [0.18797354],\n",
       "       [0.19064342],\n",
       "       [0.18831028],\n",
       "       [0.18944077],\n",
       "       [0.19389056],\n",
       "       [0.18073361],\n",
       "       [0.17702946],\n",
       "       [0.18215274],\n",
       "       [0.16969333],\n",
       "       [0.15992784],\n",
       "       [0.14525556],\n",
       "       [0.13587492],\n",
       "       [0.16074564],\n",
       "       [0.15829224],\n",
       "       [0.15848467],\n",
       "       [0.16211666],\n",
       "       [0.16254961],\n",
       "       [0.12471437],\n",
       "       [0.13931449],\n",
       "       [0.13852075],\n",
       "       [0.14530367],\n",
       "       [0.15146121],\n",
       "       [0.15263981],\n",
       "       [0.14900782],\n",
       "       [0.1434997 ],\n",
       "       [0.15088394],\n",
       "       [0.15538184],\n",
       "       [0.15759471],\n",
       "       [0.15990379],\n",
       "       [0.15143716],\n",
       "       [0.15292844],\n",
       "       [0.14999399],\n",
       "       [0.16216476],\n",
       "       [0.15865304],\n",
       "       [0.15475646],\n",
       "       [0.1801804 ],\n",
       "       [0.18304269],\n",
       "       [0.18323512],\n",
       "       [0.19461215],\n",
       "       [0.20165965],\n",
       "       [0.20180397],\n",
       "       [0.19386651],\n",
       "       [0.1925917 ],\n",
       "       [0.19023452],\n",
       "       [0.19374624],\n",
       "       [0.19203848],\n",
       "       [0.19352977],\n",
       "       [0.19261575],\n",
       "       [0.19384245],\n",
       "       [0.19648827],\n",
       "       [0.19415514],\n",
       "       [0.19874925],\n",
       "       [0.20177992],\n",
       "       [0.20202044],\n",
       "       [0.2033193 ],\n",
       "       [0.1991822 ],\n",
       "       [0.20355983],\n",
       "       [0.20567649],\n",
       "       [0.20490679],\n",
       "       [0.20247745],\n",
       "       [0.19761876],\n",
       "       [0.19860493]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total rows:816\n",
    "# adding every 100th row to dataY...so dataY will be 816-100 = 716\n",
    "import numpy\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):  # 841-1-1 i will go from 0 to 838\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "        dataX.append(a)    ## first 100 element will go in X  [0:99,]\n",
    "        dataY.append(dataset[i + time_step, 0])  # [100,] in Y \n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
    "time_step = 100  # hyper-paramter or tuning parameter \n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, ytest = create_dataset(test_data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06708358 0.06080577 0.06212868 ... 0.01079976 0.00817799 0.00793746]\n",
      " [0.06080577 0.06212868 0.05493686 ... 0.00817799 0.00793746 0.01164161]\n",
      " [0.06212868 0.05493686 0.05325316 ... 0.00793746 0.01164161 0.00926037]\n",
      " ...\n",
      " [0.32093806 0.32839447 0.30311485 ... 0.1991822  0.20355983 0.20567649]\n",
      " [0.32839447 0.30311485 0.29852075 ... 0.20355983 0.20567649 0.20490679]\n",
      " [0.30311485 0.29852075 0.31694528 ... 0.20567649 0.20490679 0.20247745]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train) ### per row will take 100 feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(716, 100)\n",
      "(716,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(y_train.shape) ### see the shape here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 100)\n",
      "(340,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape), print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the Stacked LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(50,return_sequences=True,input_shape=(100,1))) # layer 1\n",
    "# 50 neurons, return_sequence is passed again as input, 100 rows 1 col at a time\n",
    "model.add(LSTM(50,return_sequences=True)) # layer 2\n",
    "model.add(LSTM(50)) # layer 3\n",
    "model.add(Dense(1)) # output laayer\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 50)           10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 50,851\n",
      "Trainable params: 50,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "816/64 # 64 batches means 1 batch will have 12 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001E4D3FBD0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001E4D3FBD0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0109WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001E4E527DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001E4E527DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 0.0109 - val_loss: 0.0560\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.0022 - val_loss: 0.0217\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6.6607e-04 - val_loss: 0.0222\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 4.0960e-04 - val_loss: 0.0202\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.6443e-04 - val_loss: 0.0194\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.5706e-04 - val_loss: 0.0190\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.3572e-04 - val_loss: 0.0185\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.1324e-04 - val_loss: 0.0182\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.1870e-04 - val_loss: 0.0177\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.9228e-04 - val_loss: 0.0170\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.8954e-04 - val_loss: 0.0164\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.7464e-04 - val_loss: 0.0158\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.8541e-04 - val_loss: 0.0152\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.7417e-04 - val_loss: 0.0149\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.4275e-04 - val_loss: 0.0145\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.2690e-04 - val_loss: 0.0146\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.1474e-04 - val_loss: 0.0145\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.0915e-04 - val_loss: 0.0165\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.2715e-04 - val_loss: 0.0149\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.9776e-04 - val_loss: 0.0150\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 1.9479e-04 - val_loss: 0.0156\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.0271e-04 - val_loss: 0.0152\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.8815e-04 - val_loss: 0.0151\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.8194e-04 - val_loss: 0.0157\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.8211e-04 - val_loss: 0.0156\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.8891e-04 - val_loss: 0.0153\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.8169e-04 - val_loss: 0.0145\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.9616e-04 - val_loss: 0.0147\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 1.8017e-04 - val_loss: 0.0145\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.6972e-04 - val_loss: 0.0150\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 1.6898e-04 - val_loss: 0.0155\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 1.7933e-04 - val_loss: 0.0151\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.9416e-04 - val_loss: 0.0153\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 2.2661e-04 - val_loss: 0.0145\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 1.7610e-04 - val_loss: 0.0142\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 1.6335e-04 - val_loss: 0.0139\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.6430e-04 - val_loss: 0.0141\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.6214e-04 - val_loss: 0.0144\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 1.6359e-04 - val_loss: 0.0143\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 1.5892e-04 - val_loss: 0.0145\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.6988e-04 - val_loss: 0.0149\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.6384e-04 - val_loss: 0.0146\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.7265e-04 - val_loss: 0.0139\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.9314e-04 - val_loss: 0.0137\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.6820e-04 - val_loss: 0.0137\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.4942e-04 - val_loss: 0.0138\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.5429e-04 - val_loss: 0.0138\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.5309e-04 - val_loss: 0.0135\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.4943e-04 - val_loss: 0.0142\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.4786e-04 - val_loss: 0.0137\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.5088e-04 - val_loss: 0.0136\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.5449e-04 - val_loss: 0.0136\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.4744e-04 - val_loss: 0.0127\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.4693e-04 - val_loss: 0.0130\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.3685e-04 - val_loss: 0.0132\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.3715e-04 - val_loss: 0.0132\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 1.3570e-04 - val_loss: 0.0126\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.3363e-04 - val_loss: 0.0130\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.5756e-04 - val_loss: 0.0134\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.3823e-04 - val_loss: 0.0125\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 88ms/step - loss: 1.3201e-04 - val_loss: 0.0129\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.3083e-04 - val_loss: 0.0121\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.3599e-04 - val_loss: 0.0129\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.4185e-04 - val_loss: 0.0121e\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.4394e-04 - val_loss: 0.0117\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.2587e-04 - val_loss: 0.0118\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.2565e-04 - val_loss: 0.0129\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.8814e-04 - val_loss: 0.0102\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.3389e-04 - val_loss: 0.0107\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 1.2132e-04 - val_loss: 0.0110\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.4467e-04 - val_loss: 0.0102\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.3049e-04 - val_loss: 0.0108\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1894e-04 - val_loss: 0.0113\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.1795e-04 - val_loss: 0.0115\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1937e-04 - val_loss: 0.0103\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.2470e-04 - val_loss: 0.0089\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.6245e-04 - val_loss: 0.0099\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1783e-04 - val_loss: 0.0096\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1934e-04 - val_loss: 0.0096\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.1685e-04 - val_loss: 0.0090\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.0894e-04 - val_loss: 0.0093\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0998e-04 - val_loss: 0.0092\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.0731e-04 - val_loss: 0.0086\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.1490e-04 - val_loss: 0.0087\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 1.1496e-04 - val_loss: 0.0083\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.0445e-04 - val_loss: 0.0082\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 1.0708e-04 - val_loss: 0.0092\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0583e-04 - val_loss: 0.0082\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.7273e-05 - val_loss: 0.0080\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0602e-04 - val_loss: 0.0079\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0239e-04 - val_loss: 0.0076\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.6073e-05 - val_loss: 0.0076\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0050e-04 - val_loss: 0.0078\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.4206e-05 - val_loss: 0.0076\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 9.2653e-05 - val_loss: 0.0075\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.1571e-04 - val_loss: 0.0072\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 9.9770e-05 - val_loss: 0.0071\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 9.4795e-05 - val_loss: 0.0071\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 9.1609e-05 - val_loss: 0.0072\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 9.9563e-05 - val_loss: 0.0070\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.7993e-05 - val_loss: 0.0070\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.9540e-05 - val_loss: 0.0069\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.9117e-05 - val_loss: 0.0069\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.8095e-05 - val_loss: 0.0073\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 9.5343e-05 - val_loss: 0.0067\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.1664e-04 - val_loss: 0.0065\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.4994e-05 - val_loss: 0.0063\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.6443e-05 - val_loss: 0.0071\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.0024e-04 - val_loss: 0.0062\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 9.5751e-05 - val_loss: 0.0063\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.8243e-05 - val_loss: 0.0062\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 8.6039e-05 - val_loss: 0.0063\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 9.5617e-05 - val_loss: 0.0060\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 8.1870e-05 - val_loss: 0.0065\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 9.3836e-05 - val_loss: 0.0062\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 8.1498e-05 - val_loss: 0.0060\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 7.9455e-05 - val_loss: 0.0058\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 8.5614e-05 - val_loss: 0.0070\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1400e-04 - val_loss: 0.0059\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 8.8475e-05 - val_loss: 0.0056\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.0106e-05 - val_loss: 0.0063\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.9841e-05 - val_loss: 0.0062\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.4120e-05 - val_loss: 0.0056\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.0830e-05 - val_loss: 0.0053\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.6961e-05 - val_loss: 0.0055\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.8002e-05 - val_loss: 0.0056\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 7.4976e-05 - val_loss: 0.0056\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 7.6423e-05 - val_loss: 0.0052\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 7.6783e-05 - val_loss: 0.0053\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.5766e-05 - val_loss: 0.0055\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.3868e-05 - val_loss: 0.0053\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.5788e-05 - val_loss: 0.0053\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.0804e-04 - val_loss: 0.0049\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 9.0389e-05 - val_loss: 0.0050\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.0173e-05 - val_loss: 0.0049\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.5171e-05 - val_loss: 0.0049\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.9211e-05 - val_loss: 0.0049\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.1153e-05 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.5290e-05 - val_loss: 0.0050\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 7.4066e-05 - val_loss: 0.0046\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 7.1716e-05 - val_loss: 0.0049\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.7791e-05 - val_loss: 0.0047\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.2670e-05 - val_loss: 0.0045\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.0810e-05 - val_loss: 0.0046\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.8083e-05 - val_loss: 0.0045\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 7.1751e-05 - val_loss: 0.0045\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 7.1102e-05 - val_loss: 0.0045\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 7.1715e-05 - val_loss: 0.0047\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.9225e-05 - val_loss: 0.0043\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.6966e-05 - val_loss: 0.0044\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.9823e-05 - val_loss: 0.0044\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 7.1304e-05 - val_loss: 0.0047\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.1629e-05 - val_loss: 0.0043\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.0743e-05 - val_loss: 0.0044\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.8926e-05 - val_loss: 0.0047\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.6041e-05 - val_loss: 0.0042\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.4115e-05 - val_loss: 0.0043\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6.9267e-05 - val_loss: 0.0041\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 7.0523e-05 - val_loss: 0.0041\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.5752e-05 - val_loss: 0.0046\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 7.4271e-05 - val_loss: 0.0042\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.7761e-05 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6.3390e-05 - val_loss: 0.0041\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.2415e-05 - val_loss: 0.0041\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.3858e-05 - val_loss: 0.0040\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.9279e-05 - val_loss: 0.0043\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.6190e-05 - val_loss: 0.0040\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.6444e-05 - val_loss: 0.0039\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.7322e-05 - val_loss: 0.0041\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6.8642e-05 - val_loss: 0.0039\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6.4242e-05 - val_loss: 0.0039\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6.2779e-05 - val_loss: 0.0039\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6.4339e-05 - val_loss: 0.0042\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6.2589e-05 - val_loss: 0.0037\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 7.7848e-05 - val_loss: 0.0038\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.0050e-05 - val_loss: 0.0039\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.4493e-05 - val_loss: 0.0037\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.6593e-05 - val_loss: 0.0037\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.8250e-05 - val_loss: 0.0036\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.4154e-05 - val_loss: 0.0039\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 8.2564e-05 - val_loss: 0.0037\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 7.2948e-05 - val_loss: 0.0037\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.8926e-05 - val_loss: 0.0039\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.1696e-05 - val_loss: 0.0042\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.7635e-05 - val_loss: 0.0036\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.0539e-05 - val_loss: 0.0036\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.1497e-05 - val_loss: 0.0037\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.5051e-05 - val_loss: 0.0036\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5.8918e-05 - val_loss: 0.0038\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.4558e-05 - val_loss: 0.0037\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 5.9817e-05 - val_loss: 0.0036\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5.9143e-05 - val_loss: 0.0036\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.0305e-05 - val_loss: 0.0035\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6.2359e-05 - val_loss: 0.0037\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.3937e-05 - val_loss: 0.0035\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5.6019e-05 - val_loss: 0.0035\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.2481e-05 - val_loss: 0.0034\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5.6090e-05 - val_loss: 0.0036\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.6311e-05 - val_loss: 0.0035\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.9391e-05 - val_loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e4de509548>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=200,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001E4EAF67438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001E4EAF67438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test) # scaled output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_1 = model.predict(X_train)\n",
    "test_predict_1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transformback to original form\n",
    "train_predict=scaler.inverse_transform(train_predict_1)\n",
    "test_predict=scaler.inverse_transform(test_predict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007369474552611086"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calculate RMSE performance metrics\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "math.sqrt(mean_squared_error(y_train,train_predict_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058965687541481314"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Data RMSE\n",
    "math.sqrt(mean_squared_error(ytest,test_predict_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Closing</th>\n",
       "      <th>CLosing1</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011642</td>\n",
       "      <td>95.18</td>\n",
       "      <td>94.992767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009260</td>\n",
       "      <td>94.19</td>\n",
       "      <td>94.203590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006975</td>\n",
       "      <td>93.24</td>\n",
       "      <td>94.108269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005725</td>\n",
       "      <td>92.72</td>\n",
       "      <td>94.097511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005893</td>\n",
       "      <td>92.79</td>\n",
       "      <td>93.935928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Closing  CLosing1  Prediction\n",
       "0  0.011642     95.18   94.992767\n",
       "1  0.009260     94.19   94.203590\n",
       "2  0.006975     93.24   94.108269\n",
       "3  0.005725     92.72   94.097511\n",
       "4  0.005893     92.79   93.935928"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_train=pd.DataFrame(y_train,columns=[\"Closing\"])\n",
    "Final_train['CLosing1']=scaler.inverse_transform(Final_train[[\"Closing\"]])\n",
    "Final_train[\"Prediction\"]=train_predict\n",
    "Final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Closing</th>\n",
       "      <th>CLosing1</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.063139</td>\n",
       "      <td>116.59</td>\n",
       "      <td>114.826393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.069056</td>\n",
       "      <td>119.05</td>\n",
       "      <td>115.158012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.077883</td>\n",
       "      <td>122.72</td>\n",
       "      <td>116.331940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.078749</td>\n",
       "      <td>123.08</td>\n",
       "      <td>118.558479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.078413</td>\n",
       "      <td>122.94</td>\n",
       "      <td>120.823135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Closing  CLosing1  Prediction\n",
       "335  0.063139    116.59  114.826393\n",
       "336  0.069056    119.05  115.158012\n",
       "337  0.077883    122.72  116.331940\n",
       "338  0.078749    123.08  118.558479\n",
       "339  0.078413    122.94  120.823135"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_test=pd.DataFrame(ytest,columns=[\"Closing\"])\n",
    "Final_test['CLosing1']=scaler.inverse_transform(Final_test[[\"Closing\"]])\n",
    "Final_test[\"Prediction\"]=test_predict\n",
    "Final_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
